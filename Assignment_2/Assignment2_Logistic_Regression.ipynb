{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install otter-grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"Assignment2_Logistic_Regression.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS385/CSD3185/CSD3186: Assignment 2 Logistic Regression\n",
    "\n",
    "## Deliverables\n",
    "Your submission for this assignment should be __ONE__ file - a zip file generated by the `grader.export(_)` function down below under 'Submission'. This function will auto create the zip file which contains this particular completed notebook file for you.  \n",
    "\n",
    "Subsequently, rename your zip file like this: __coursecode_A2_your_full_name.zip__  \n",
    "Eg. CS385_A2_john_doe.zip  \n",
    "\n",
    "To complete this assignment, you should follow instructions in Section Tasks.\n",
    "\n",
    "## IMPORTANT! READ THIS BEFORE STARTING...\n",
    "- DO NOT delete existing cells, but you can add more cells in between.\n",
    "- DO NOT modify the content of the existing cells unless otherwise stated.\n",
    "- Run the cell with `grader.check(_)` to check your solutions whenever you have completed each tasks.\n",
    "- Follow the file naming convention for the zip file as spelled out above strictly.\n",
    "- DO NOT rename this notebook file. It shall be 'Assignment2_Logistic_Regression.ipynb'. \n",
    "\n",
    "Please adhere strictly to the instructions as stated above as failure to do so might result in deduction of marks by the autograder.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Covered\n",
    "**Logistic Regression** is a Machine Learning classification algorithm that is used to predict the probability of a categorical dependent variable. In logistic regression, the dependent variable takes on one of two possible values, typically coded as 1 (e.g., yes, success, etc.) or 0 (e.g., no, failure, etc.). The logistic regression model estimates the probability of the dependent variable being 1, denoted as $P(Y=1)$, based on the input features ($X$).\n",
    "\n",
    "In this notebook, We'll use the <a href=\"https://archive.ics.uci.edu/dataset/222/bank+marketing\">Bank Marketing dataset</a> from the UCI Machine Learning Repository, which focuses on direct marketing campaigns conducted via phone calls by a Portuguese banking institution. The objective is to predict whether a client will subscribe to a term deposit (coded as 1 for yes or 0 for no).\n",
    "\n",
    "**Notebook Structure**:\n",
    "1. Loading and Inspecting the Dataset [_5 marks_]\n",
    "2. Exploring the Dataset (Part I) [_5 marks_]\n",
    "3. Exploring the Dataset (part II) [_15 marks_]\n",
    "4. Create Dummy variables \n",
    "5. Random Over-sampling of the Minority Class [_25 marks_]\n",
    "6. Logistic Regression Model Fitting [_20 marks_]\n",
    "7. Grid Searching for Hyperparameter Selection [_30 marks_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Overview**   \n",
    "|**Variable**     |**Description**                         |**Type**                                     |\n",
    "|:-----------------------|:-------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------|\n",
    "| `age`                | Age                                                                                                                     | Numeric                                                                                   |\n",
    "| `job`                | Type of job                                                                                                             | Categorical: ‚Äúadmin‚Äù, ‚Äúblue-collar‚Äù, ‚Äúentrepreneur‚Äù, ‚Äúhousemaid‚Äù, ‚Äúmanagement‚Äù, ‚Äúretired‚Äù, ‚Äúself-employed‚Äù, ‚Äúservices‚Äù, ‚Äústudent‚Äù, ‚Äútechnician‚Äù, ‚Äúunemployed‚Äù, ‚Äúunknown‚Äù |\n",
    "| `marital`            | Marital status                                                                                                          | Categorical: ‚Äúdivorced‚Äù, ‚Äúmarried‚Äù, ‚Äúsingle‚Äù, ‚Äúunknown‚Äù                                   |\n",
    "| `education`          | Education level                                                                                                         | Categorical: ‚Äúbasic.4y‚Äù, ‚Äúbasic.6y‚Äù, ‚Äúbasic.9y‚Äù, ‚Äúhigh.school‚Äù, ‚Äúilliterate‚Äù, ‚Äúprofessional.course‚Äù, ‚Äúuniversity.degree‚Äù, ‚Äúunknown‚Äù |\n",
    "| `default`            | Has credit in default?                                                                                                  | Categorical: ‚Äúno‚Äù, ‚Äúyes‚Äù, ‚Äúunknown‚Äù                                                      |\n",
    "| `housing`            | Has housing loan?                                                                                                       | Categorical: ‚Äúno‚Äù, ‚Äúyes‚Äù, ‚Äúunknown‚Äù                                                      |\n",
    "| `loan`               | Has personal loan?                                                                                                      | Categorical: ‚Äúno‚Äù, ‚Äúyes‚Äù, ‚Äúunknown‚Äù                                                      |\n",
    "| `contact`            | Contact communication type                                                                                              | Categorical: ‚Äúcellular‚Äù, ‚Äútelephone‚Äù                                                     |\n",
    "| `month`              | Last contact month of year                                                                                              | Categorical: ‚Äújan‚Äù, ‚Äúfeb‚Äù, ‚Äúmar‚Äù, ‚Ä¶, ‚Äúnov‚Äù, ‚Äúdec‚Äù                                        |\n",
    "| `day_of_week`        | Last contact day of the week                                                                                            | Categorical: ‚Äúmon‚Äù, ‚Äútue‚Äù, ‚Äúwed‚Äù, ‚Äúthu‚Äù, ‚Äúfri‚Äù                                           |\n",
    "| `duration`           | Last contact duration in seconds. Note: affects the output target and should be discarded for realistic models.          | Numeric                                                                                   |\n",
    "| `campaign`           | Number of contacts performed during this campaign for this client                                                       | Numeric                                                                                   |\n",
    "| `pdays`              | Number of days since last contact from a previous campaign (999 = not previously contacted)                             | Numeric                                                                                   |\n",
    "| `previous`           | Number of contacts performed before this campaign for this client                                                       | Numeric                                                                                   |\n",
    "| `poutcome`           | Outcome of the previous marketing campaign                                                                              | Categorical: ‚Äúfailure‚Äù, ‚Äúnonexistent‚Äù, ‚Äúsuccess‚Äù                                         |\n",
    "| `emp.var.rate`       | Employment variation rate                                                                                               | Numeric                                                                                   |\n",
    "| `cons.price.idx`     | Consumer price index                                                                                                   | Numeric                                                                                   |\n",
    "| `cons.conf.idx`      | Consumer confidence index                                                                                              | Numeric                                                                                   |\n",
    "| `euribor3m`          | Euribor 3-month rate                                                                                                   | Numeric                                                                                   |\n",
    "| `nr.employed`        | Number of employees                                                                                                    | Numeric                                                                                   |\n",
    "| **Target variable**  | **Description**                                                                                                         | **Type**                                                                                  |\n",
    "| `y`                  | Has the client subscribed to a term deposit?                                                                           | Binary: ‚Äú1‚Äù (Yes), ‚Äú0‚Äù (No)                                                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides the bank customers‚Äô information. It includes 41,188 records and 21 fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** To ensure the grid search section in Part 7 runs without errors, please upgrade your scikit-learn package to version 1.6.1. You only need to run the following command once. After successfully upgrading, you can comment out the line to avoid reinstallation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user --upgrade scikit-learn==1.6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#plt.rc(\"font\", size=14)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/__init__.py:73\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# `_distributor_init` allows distributors to run custom init code.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# For instance, for the Windows wheel, this is used to pre-load the\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# vcomp shared library runtime for OpenMP embedded in the sklearn/.libs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401 E402\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     __check_build,\n\u001b[1;32m     71\u001b[0m     _distributor_init,\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[1;32m     76\u001b[0m _submodules \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompose\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m ]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:19\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HTMLDocumentationLinkMixin, estimator_html_repr\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_metadata_requests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _MetadataRequester, _routing_enabled\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_parameter_constraints\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/__init__.py:15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _joblib, metadata_routing\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bunch\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_chunking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batches, gen_even_slices\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Make _safe_indexing importable from here for backward compat as this particular\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# helper is considered semi-private and typically very useful for third-party\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# libraries that want to comply with scikit-learn's estimator API. In particular,\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# _safe_indexing was included in our public API documentation despite the leading\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# `_` in its name.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/_chunking.py:11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Interval, validate_params\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchunk_generator\u001b[39m(gen, chunksize):\n\u001b[1;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Chunk generator, ``gen`` into lists of length ``chunksize``. The last\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    chunk may have a length less than ``chunksize``.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m csr_matrix, issparse\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mInvalidParameterError\u001b[39;00m(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m     21\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m    does not have a valid type or value.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config \u001b[38;5;28;01mas\u001b[39;00m _get_config\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_force_all_finite\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ComplexWarning, _preserve_dia_indices_dtype\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/_array_api.py:17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version\n\u001b[1;32m     19\u001b[0m _NUMPY_NAMESPACE_NAMES \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray_api_compat.numpy\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myield_namespaces\u001b[39m(include_numpy_namespaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/fixes.py:17\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/stats/__init__.py:606\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    601\u001b[0m \n\u001b[1;32m    602\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_warnings_errors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[1;32m    605\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[0;32m--> 606\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_py\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_variation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/stats/_stats_py.py:49\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mspecial\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linalg\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _mstats_basic \u001b[38;5;28;01mas\u001b[39;00m mstats_basic\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats_mstats_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (_find_repeats, linregress, theilslopes,\n\u001b[1;32m     52\u001b[0m                                    siegelslopes)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/stats/distributions.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_levy_stable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m levy_stable\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_discrete_distns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_entropy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m entropy\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# For backwards compatibility e.g. pymc expects distributions.__all__.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrv_discrete\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrv_continuous\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrv_histogram\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# noqa: F405\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/stats/_entropy.py:169\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m \u001b[38;5;129m@_axis_nan_policy_factory\u001b[39m(\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: x, n_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, result_to_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: (x,),\n\u001b[1;32m    171\u001b[0m     too_small\u001b[38;5;241m=\u001b[39m_differential_entropy_is_too_small\n\u001b[1;32m    172\u001b[0m )\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdifferential_entropy\u001b[39m(\n\u001b[1;32m    174\u001b[0m     values: np\u001b[38;5;241m.\u001b[39mtyping\u001b[38;5;241m.\u001b[39mArrayLike,\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    176\u001b[0m     window_length: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    177\u001b[0m     base: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    178\u001b[0m     axis: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m    179\u001b[0m     method: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mnumber \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Given a sample of a distribution, estimate the differential entropy.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m    Several estimation methods are available using the `method` parameter. By\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m \n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(values)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:608\u001b[0m, in \u001b[0;36m_axis_nan_policy_factory.<locals>.axis_nan_policy_decorator\u001b[0;34m(hypotest_fun_in)\u001b[0m\n\u001b[1;32m    605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tuple_to_result(\u001b[38;5;241m*\u001b[39mres)\n\u001b[1;32m    607\u001b[0m _axis_parameter_doc, _axis_parameter \u001b[38;5;241m=\u001b[39m _get_axis_params(default_axis)\n\u001b[0;32m--> 608\u001b[0m doc \u001b[38;5;241m=\u001b[39m FunctionDoc(axis_nan_policy_wrapper)\n\u001b[1;32m    609\u001b[0m parameter_names \u001b[38;5;241m=\u001b[39m [param\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m doc[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameters\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxis\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m parameter_names:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/_lib/_docscrape.py:570\u001b[0m, in \u001b[0;36mFunctionDoc.__init__\u001b[0;34m(self, func, role, doc, config)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo function or docstring given\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    569\u001b[0m     doc \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetdoc(func) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 570\u001b[0m NumpyDocString\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, doc, config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/_lib/_docscrape.py:148\u001b[0m, in \u001b[0;36mNumpyDocString.__init__\u001b[0;34m(self, docstring, config)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parsed_data \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msections)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse()\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParseError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    150\u001b[0m     e\u001b[38;5;241m.\u001b[39mdocstring \u001b[38;5;241m=\u001b[39m orig_docstring\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/_lib/_docscrape.py:373\u001b[0m, in \u001b[0;36mNumpyDocString._parse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_parse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_doc\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_summary()\n\u001b[1;32m    375\u001b[0m     sections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_sections())\n\u001b[1;32m    376\u001b[0m     section_names \u001b[38;5;241m=\u001b[39m {section \u001b[38;5;28;01mfor\u001b[39;00m section, content \u001b[38;5;129;01min\u001b[39;00m sections}\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/_lib/_docscrape.py:369\u001b[0m, in \u001b[0;36mNumpyDocString._parse_summary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m summary\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_at_section():\n\u001b[0;32m--> 369\u001b[0m     \u001b[38;5;28mself\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtended Summary\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_to_next_section()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/scipy/_lib/_docscrape.py:195\u001b[0m, in \u001b[0;36mNumpyDocString._read_to_next_section\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc[i:\u001b[38;5;28mlen\u001b[39m(doc)\u001b[38;5;241m-\u001b[39mj]\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_to_next_section\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    196\u001b[0m     section \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_doc\u001b[38;5;241m.\u001b[39mread_to_next_empty_line()\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_at_section() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_doc\u001b[38;5;241m.\u001b[39meof():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "#plt.rc(\"font\", size=14)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "#sns.set(style=\"white\")\n",
    "#sns.set(style=\"whitegrid\", color_codes=True)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading and Inspecting the Dataset\n",
    "\n",
    "In this section, we will load the dataset into a pandas DataFrame and perform a quick inspection. This step ensures the data is properly loaded and gives an initial understanding of its structure.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Load the dataset <code>\"banking.csv\"</code> into a pandas DataFrame called <code>data</code>. \n",
    "- Display the first 5 rows of <code>data</code> using the <code>head()</code> method.\n",
    "- Provide summary statistics for numeric columns in the DataFrame using the `describe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace ... with your code\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"banking.csv\")\n",
    "\n",
    "# Display the first 5 rows\n",
    "head_data = data[:5]\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(head_data)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Provide summary statisitcis for numberical columns in the dataframe\n",
    "print(\"Summary statistics for numerical columns:\")\n",
    "data_description = data.describe()\n",
    "print(data_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>task1</pre></strong> passed! üôå</p>"
      ],
      "text/plain": [
       "task1 results: All test cases passed!"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"task1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Dataset (part i)\n",
    "In this section, we will explore the dataset further to identify missing values, understand categorical variables, and examine the structure of the data. This step helps in identifying potential issues and gaining insights into the dataset.\n",
    "\n",
    "### Instructions\n",
    "- **Identify Missing Values**: Use an appropriate pandas method to calculate and display the number of missing values in each column.\n",
    "- **Understand Dataset Structure**: Verify the dataset's shape by printing the number of rows and columns.\n",
    "- **Check Unique Categories**: Display the unique categories in the `education` column to understand its categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace ... with your code\n",
    "# Inspect the number of missing values in each column\n",
    "missing_values = data.isna().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Shape of the dataframe\n",
    "data_shape = data.shape\n",
    "print(\"\\nShape of the DataFrame:\")\n",
    "print(data_shape)\n",
    "\n",
    "# Check how many categories are in the education column\n",
    "education_categories = data[\"education\"].unique()\n",
    "print(\"\\nUnique categories in the 'education' column:\")\n",
    "print(education_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>task2</pre></strong> passed! üçÄ</p>"
      ],
      "text/plain": [
       "task2 results: All test cases passed!"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"task2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The education column of the dataset has many categories and we need to reduce the categories for a better modelling. Let us group ‚Äúbasic.4y‚Äù, ‚Äúbasic.9y‚Äù and ‚Äúbasic.6y‚Äù together and call them ‚Äúbasic‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['education']=np.where(data['education'] =='basic.9y', 'Basic', data['education'])\n",
    "data['education']=np.where(data['education'] =='basic.6y', 'Basic', data['education'])\n",
    "data['education']=np.where(data['education'] =='basic.4y', 'Basic', data['education'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After grouping, it reduced to six education categories: 'Basic', 'unknown', 'university.degree', 'high.school','professional.course', 'illiterate'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploring the Dataset (part ii)\n",
    "In this section, we will analyze the target variable y to understand its distribution and derive some basic insights. This analysis helps assess class imbalance and potential relationships between the target and other variables.\n",
    "\n",
    "## Instructions\n",
    "- Display the value counts of the target variable y using the `value_counts()` method\n",
    "- Calculate the number and percentage of rows where y is 0 (no subscription) and 1 (subscription).\n",
    "- Display the mean of numeric columns grouped by the target variable y using the `groupby()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a count plot for the target variable\n",
    "sns.countplot(x='y', data=data, palette='hls')  \n",
    "plt.title(\"Count Plot for Target Variable 'y'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace ... with your code\n",
    "# Display value counts for the target variable\n",
    "y_value_counts = data[\"y\"].value_counts()\n",
    "print(\"Value counts for the target variable 'y':\")\n",
    "print(y_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace ... with your code\n",
    "# Calculate subscription statistics\n",
    "count_no_sub = y_value_counts[0]\n",
    "count_sub = y_value_counts[1]\n",
    "\n",
    "pct_of_no_sub = y_value_counts[0] / y_value_counts.sum()\n",
    "pct_of_sub = y_value_counts[1] / y_value_counts.sum()\n",
    "\n",
    "print(f\"Percentage of no subscription: {pct_of_no_sub * 100:.2f}%\")\n",
    "print(f\"Percentage of subscription: {pct_of_sub * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp_var_rate</th>\n",
       "      <th>cons_price_idx</th>\n",
       "      <th>cons_conf_idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr_employed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.911185</td>\n",
       "      <td>220.844807</td>\n",
       "      <td>2.633085</td>\n",
       "      <td>984.113878</td>\n",
       "      <td>0.132374</td>\n",
       "      <td>0.248875</td>\n",
       "      <td>93.603757</td>\n",
       "      <td>-40.593097</td>\n",
       "      <td>3.811491</td>\n",
       "      <td>5176.166600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.913147</td>\n",
       "      <td>553.191164</td>\n",
       "      <td>2.051724</td>\n",
       "      <td>792.035560</td>\n",
       "      <td>0.492672</td>\n",
       "      <td>-1.233448</td>\n",
       "      <td>93.354386</td>\n",
       "      <td>-39.789784</td>\n",
       "      <td>2.123135</td>\n",
       "      <td>5095.115991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         age    duration  campaign       pdays  previous  emp_var_rate  \\\n",
       "y                                                                        \n",
       "0  39.911185  220.844807  2.633085  984.113878  0.132374      0.248875   \n",
       "1  40.913147  553.191164  2.051724  792.035560  0.492672     -1.233448   \n",
       "\n",
       "   cons_price_idx  cons_conf_idx  euribor3m  nr_employed  \n",
       "y                                                         \n",
       "0       93.603757     -40.593097   3.811491  5176.166600  \n",
       "1       93.354386     -39.789784   2.123135  5095.115991  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace ... with your code\n",
    "# Display the mean of numeric columns grouped by the target variable\n",
    "grouped_means = data.select_dtypes([np.number]).groupby(by=[\"y\"]).mean()\n",
    "grouped_means\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>task3</pre></strong> passed! üåà</p>"
      ],
      "text/plain": [
       "task3 results: All test cases passed!"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"task3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Based on the outputs above, what are some observations? Please share some of your findings.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**  \n",
    "- Our classes are imbalanced, and the ratio of no-subscription to subscription instances is 89:11.\n",
    "- The average age of customers who bought the term deposit is higher than that of the customers who didn‚Äôt.  \n",
    "- The pdays (days since the customer was last contacted) is understandably lower for the customers who bought it. The lower the pdays, the better the memory of the last call and hence the better chances of a sale.  \n",
    "- Surprisingly, campaigns (number of contacts or calls made during the current campaign) are lower for customers who bought the term deposit.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Create dummy variables**  \n",
    "In this section, we will learn how to transform categorical variables into dummy variables, which are binary representations. **The codes in this section are provided and no modification is required.** Dummy variables are essential for preparing data for machine learning models that require numeric input.\n",
    "\n",
    "**Example:**\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'green', 'red', 'green']\n",
    "})\n",
    "\n",
    "# Create dummy variables\n",
    "dummy_df = pd.get_dummies(data['color'], prefix='color')\n",
    "\n",
    "print(dummy_df)\n",
    "\n",
    "```\n",
    "**Output:**\n",
    "```\n",
    "   color_blue  color_green  color_red\n",
    "0       False        False       True\n",
    "1        True        False      False\n",
    "2       False         True      False\n",
    "3       False        False       True\n",
    "4       False         True      False\n",
    "```\n",
    "**Explanation:**\n",
    "* pd.get_dummies() takes the color column and creates a binary column for each unique category (blue, green, red).\n",
    "* Each row has a True under the corresponding color and False for the rest. You can change the dtype to int so that it returns 1 and 0 for each column, for example:\n",
    "  ```python\n",
    "  dummy_df = pd.get_dummies(data['color'], prefix='color', dtype=int)\n",
    "  ```\n",
    "* The prefix='color' argument ensures that the dummy columns are prefixed with the word color.\n",
    "\n",
    "This approach is widely used to convert categorical variables into a format suitable for machine learning models. To learn more about pd.get_dummies() and its usage, refer to the [pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars=['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\n",
    "for var in cat_vars:\n",
    "    cat_list='var'+'_'+var\n",
    "    cat_list = pd.get_dummies(data[var], prefix=var, dtype = int) # by default, it returns True or False\n",
    "    data1=data.join(cat_list)\n",
    "    \n",
    "    \n",
    "cat_vars=['job','marital','education','default','housing','loan','contact','month','day_of_week','poutcome']\n",
    "data_vars=data1.columns.values.tolist()\n",
    "to_keep=[i for i in data_vars if i not in cat_vars]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final data columns will be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['age', 'duration', 'campaign', 'pdays', 'previous', 'emp_var_rate',\n",
       "       'cons_price_idx', 'cons_conf_idx', 'euribor3m', 'nr_employed', 'y',\n",
       "       'poutcome_failure', 'poutcome_nonexistent', 'poutcome_success'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final=data1[to_keep]\n",
    "data_final.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>emp_var_rate</th>\n",
       "      <th>cons_price_idx</th>\n",
       "      <th>cons_conf_idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr_employed</th>\n",
       "      <th>y</th>\n",
       "      <th>poutcome_failure</th>\n",
       "      <th>poutcome_nonexistent</th>\n",
       "      <th>poutcome_success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44</td>\n",
       "      <td>210</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>93.444</td>\n",
       "      <td>-36.1</td>\n",
       "      <td>4.963</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>93.200</td>\n",
       "      <td>-42.0</td>\n",
       "      <td>4.021</td>\n",
       "      <td>5195.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>339</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.7</td>\n",
       "      <td>94.055</td>\n",
       "      <td>-39.8</td>\n",
       "      <td>0.729</td>\n",
       "      <td>4991.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>185</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>93.075</td>\n",
       "      <td>-47.1</td>\n",
       "      <td>1.405</td>\n",
       "      <td>5099.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>92.201</td>\n",
       "      <td>-31.4</td>\n",
       "      <td>0.869</td>\n",
       "      <td>5076.2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41183</th>\n",
       "      <td>59</td>\n",
       "      <td>222</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>94.465</td>\n",
       "      <td>-41.8</td>\n",
       "      <td>4.866</td>\n",
       "      <td>5228.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41184</th>\n",
       "      <td>31</td>\n",
       "      <td>196</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.860</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41185</th>\n",
       "      <td>42</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41186</th>\n",
       "      <td>48</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.4</td>\n",
       "      <td>92.431</td>\n",
       "      <td>-26.9</td>\n",
       "      <td>0.742</td>\n",
       "      <td>5017.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41187</th>\n",
       "      <td>25</td>\n",
       "      <td>112</td>\n",
       "      <td>4</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.859</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41188 rows √ó 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  duration  campaign  pdays  previous  emp_var_rate  cons_price_idx  \\\n",
       "0       44       210         1    999         0           1.4          93.444   \n",
       "1       53       138         1    999         0          -0.1          93.200   \n",
       "2       28       339         3      6         2          -1.7          94.055   \n",
       "3       39       185         2    999         0          -1.8          93.075   \n",
       "4       55       137         1      3         1          -2.9          92.201   \n",
       "...    ...       ...       ...    ...       ...           ...             ...   \n",
       "41183   59       222         1    999         0           1.4          94.465   \n",
       "41184   31       196         2    999         0           1.1          93.994   \n",
       "41185   42        62         3    999         0           1.1          93.994   \n",
       "41186   48       200         2    999         0          -3.4          92.431   \n",
       "41187   25       112         4    999         0           1.1          93.994   \n",
       "\n",
       "       cons_conf_idx  euribor3m  nr_employed  y  poutcome_failure  \\\n",
       "0              -36.1      4.963       5228.1  0                 0   \n",
       "1              -42.0      4.021       5195.8  0                 0   \n",
       "2              -39.8      0.729       4991.6  1                 0   \n",
       "3              -47.1      1.405       5099.1  0                 0   \n",
       "4              -31.4      0.869       5076.2  1                 0   \n",
       "...              ...        ...          ... ..               ...   \n",
       "41183          -41.8      4.866       5228.1  0                 0   \n",
       "41184          -36.4      4.860       5191.0  0                 0   \n",
       "41185          -36.4      4.857       5191.0  0                 0   \n",
       "41186          -26.9      0.742       5017.5  0                 0   \n",
       "41187          -36.4      4.859       5191.0  0                 0   \n",
       "\n",
       "       poutcome_nonexistent  poutcome_success  \n",
       "0                         1                 0  \n",
       "1                         1                 0  \n",
       "2                         0                 1  \n",
       "3                         1                 0  \n",
       "4                         0                 1  \n",
       "...                     ...               ...  \n",
       "41183                     1                 0  \n",
       "41184                     1                 0  \n",
       "41185                     1                 0  \n",
       "41186                     1                 0  \n",
       "41187                     1                 0  \n",
       "\n",
       "[41188 rows x 14 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. **Random Over-sampling the minority class**  \n",
    "In this section, we will learn about over-sampling, which is the process of randomly duplicating observations from the minority class to achieve a balanced dataset.\n",
    "\n",
    "The most common approach to over-sampling is to resample with replacement. Here's how to proceed:\n",
    "\n",
    "- Import the necessary resampling module from Scikit-Learn.\n",
    "- Split the data into training and testing sets using train_test_split.\n",
    "- Separate the majority and minority classes in the training data.\n",
    "- Upsample the minority class by randomly duplicating its samples.\n",
    "- Combine the upsampled minority class with the majority class to create a balanced dataset.\n",
    "- Split the data back into features (X_train) and target (y_train).\n",
    "- Applied `StandardScaler` to standardize both training and testing features to get `X_train_scaled` and `X_test_scaled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import resample module and Scaler \n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace ... with your code\n",
    "# Split data into X and y\n",
    "X = data_final.drop(columns=\"y\")\n",
    "y = data_final[\"y\"]\n",
    "\n",
    "# Split data into training and testing sets using train_test_split\n",
    "X_train_bef, X_test, y_train_bef, y_test = train_test_split(X, y, test_size=0.3, random_state=100) \n",
    "\n",
    "# Combine X_train and y_train\n",
    "train_data = pd.concat([X_train_bef, y_train_bef], axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace ... with your code\n",
    "# Separate majority and minority classes\n",
    "# majority: train data with y == 0, minority: train data with y == 1\n",
    "majority = train_data.groupby([\"y\"]).get_group(0)\n",
    "minority = train_data.groupby([\"y\"]).get_group(1)\n",
    "\n",
    "# Upsample minority class\n",
    "minority_upsampled = resample(minority,\n",
    "                              replace=True,   # Sample with replacement\n",
    "                              n_samples=len(majority),  # Match majority class size\n",
    "                              random_state=0)  # Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace ... with your code\n",
    "# Combine majority and upsampled minority\n",
    "upsampled_data = pd.concat([majority, minority_upsampled]) \n",
    "\n",
    "# Split back into features and target\n",
    "X_train = upsampled_data.drop(columns=\"y\")\n",
    "y_train = upsampled_data[\"y\"].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace ... with your code\n",
    "# Scale features in X_train and X_test\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>task5</pre></strong> passed! üöÄ</p>"
      ],
      "text/plain": [
       "task5 results: All test cases passed!"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"task5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** You may have noticed that the over-sampling was performed only on the training data. This is important because by over-sampling only the training set, we ensure that no information from the test data is used to create synthetic observations. This prevents any potential data leakage, where information from the test set might influence the model training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Random over-sampling is a straightforward yet powerful technique for addressing class imbalance. However, it can sometimes lead to overfitting, as the model may memorize duplicated samples rather than learning to generalize effectively. To mitigate this, consider combining random over-sampling with more sophisticated approaches like SMOTE (Synthetic Minority Over-sampling Technique), which generates synthetic samples instead of duplicating existing ones. you can explore a variety of techniques and strategies in future projects when dealing with imbalanced data. Below are some popular methods for addressing class imbalance:\n",
    "\n",
    "### Alternative Resampling Techniques:\n",
    "\n",
    "**Under-sampling the Majority Class:** This involves reducing the number of samples in the majority class to match the size of the minority class. While it can balance the dataset, it risks discarding potentially valuable information from the majority class.\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique):** Unlike random over-sampling, SMOTE creates synthetic samples for the minority class by interpolating between existing samples. This helps to reduce the risk of overfitting.\n",
    "\n",
    "**ADASYN (Adaptive Synthetic Sampling):** Similar to SMOTE, ADASYN focuses on generating more synthetic samples in regions where the minority class is under-represented, improving the classifier's performance on difficult-to-learn samples.\n",
    "\n",
    "**Cluster-based Over-sampling:** Groups data into clusters before over-sampling the minority class within each cluster, ensuring better representation and diversity of the synthetic samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. **Logistic Regression Model Fitting**\n",
    "In this section, we will fit a logistic regression model to the training data and evaluate its performance on the test data. \n",
    "\n",
    "### Instructions\n",
    "- Instantiate the logistic regression model using `LogisticRegression()`.\n",
    "- Train the model using the `fit` method on the training data (`X_train_scaled` and `y_train`).\n",
    "- Make predictions on the test data (`X_test_scaled`) using the `predict` method.\n",
    "- Calculate the model's accuracy on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.85\n"
     ]
    }
   ],
   "source": [
    "# Replace ... with your code\n",
    "# Instantiate the logistic regression model\n",
    "logreg = LogisticRegression(solver=\"newton-cg\")\n",
    "\n",
    "# Fit the model to the training data\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions on the test and display the accuracy\n",
    "y_pred = logreg.predict(X_test_scaled)\n",
    "\n",
    "# Calculate and display the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy of logistic regression classifier on test set: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><strong><pre style='display: inline;'>task6</pre></strong> passed! üíØ</p>"
      ],
      "text/plain": [
       "task6 results: All test cases passed!"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader.check(\"task6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Grid Searching for selecting hyperparameters**\n",
    "\n",
    "Our initial logistic regression model achieved an accuracy of approximately 85%, which was a promising start. To further explore potential improvements, we will conduct a grid search to fine-tune the hyperparameters of the model.\n",
    "\n",
    "A **grid search** systematically tests combinations of hyperparameter values to determine the best-performing model. In this case, we'll focus on two hyperparameters of the logistic regression model:\n",
    "1. **Penalty**: Regularization technique used to avoid overfitting.\n",
    "2. **C**: Inverse of regularization strength (smaller values specify stronger regularization).\n",
    "\n",
    "### **Instructions**\n",
    "- Combine the hyperparameter lists of `penalty` and `C` into a dictionary using `dict()`, with keys as hyperparameter names and values as the corresponding lists.\n",
    "- Instantiate `GridSearchCV` with the attributes set as `estimator = lgr`, `param_grid = param_grid` and store this instance in the `grid_search` variable\n",
    "- Fit `GridSearchCV` with the training data (`X_train_scaled`, `y_train`).\n",
    "- Access the best hyperparameter combination using `.best_params_`, the best score using `.best_score_`, and the best model using `.best_estimator_`\n",
    "- Evaluate the final model on the test set using `confusion_matrix` and `classification_report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace ... with your code\n",
    "# Define hyperparameter grid\n",
    "penalty = ['l1', 'l2', 'elasticnet', 'none']\n",
    "C = [0.01, 0.1, 1, 10]\n",
    "\n",
    "# Combine the lists into a dictionary\n",
    "hyperparameters = {\"penalty\": penalty, \"C\": C}\n",
    "\n",
    "# Create logistic regression model\n",
    "lgr = LogisticRegression()\n",
    "\n",
    "# Use GridSearch\n",
    "grid_search = GridSearchCV(estimator=lgr, param_grid=hyperparameters)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Best parameters, score (accuracy by default) and best model\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "print(\"Best Model:\", best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the final model\n",
    "y_pred = grid_search.predict(X_test_scaled)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "classification_rpt = classification_report(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)  \n",
    "print(\"Classification Report:\\n\", classification_rpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"task7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicated that the optimized hyperparameters did not yield a significant improvement in performance. This outcome suggests that the default hyperparameters for logistic regression were already well-suited for the given dataset. Alternatively, it could indicate that other factors, such as the data quality, feature selection, or the nature of the problem, might play a more critical role in improving the model's performance.\n",
    "\n",
    "This exercise reinforces the importance of testing and validating assumptions during the model optimization process. While hyperparameter tuning is a valuable tool, it is not always guaranteed to lead to better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Confusion Matrix and Classification Metrics\n",
    "\n",
    "A **confusion matrix** is a performance evaluation tool for classification models. It provides detailed insights into how well a classification algorithm performs, breaking down predictions into four categories:\n",
    "\n",
    "|                     | **Predicted Positive** | **Predicted Negative** |\n",
    "|---------------------|-------------------------|-------------------------|\n",
    "| **Actual Positive** | True Positive (TP)      | False Negative (FN)     |\n",
    "| **Actual Negative** | False Positive (FP)     | True Negative (TN)      |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Terms in the Confusion Matrix\n",
    "1. **True Positive (TP)**: The model correctly predicted a positive outcome.\n",
    "2. **True Negative (TN)**: The model correctly predicted a negative outcome.\n",
    "3. **False Positive (FP)**: The model predicted positive when it is actually negative (Type I error).\n",
    "4. **False Negative (FN)**: The model predicted negative when it is actually positive (Type II error).\n",
    "\n",
    "---\n",
    "\n",
    "### How to Calculate and Interpret Metrics\n",
    "Using the values from the confusion matrix, you can derive important classification metrics:\n",
    "\n",
    "#### 1. **Accuracy** \n",
    "   - **Formula**:  \n",
    "     \n",
    "     $${Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}$$\n",
    "     \n",
    "   - **Interpretation**: The percentage of all predictions (positive and negative) that are correct.\n",
    "   - **Limitation**: Can be misleading for imbalanced datasets (e.g., when one class dominates).\n",
    "\n",
    "#### 2. Precision (Positive Predictive Value)  \n",
    "   - **Formula**:  \n",
    "     \n",
    "     $${Precision} = \\frac{TP}{TP + FP}$$\n",
    "    \n",
    "   - **Interpretation**: Of all instances predicted as positive, how many are actually positive.\n",
    "   - **Importance**: High precision reduces false positives.\n",
    "\n",
    "#### 3. Recall (Sensitivity or True Positive Rate)  \n",
    "   - **Formula**:  \n",
    "   \n",
    "     $${Recall} = \\frac{TP}{TP + FN}$$\n",
    "  \n",
    "   - **Interpretation**: Of all actual positives, how many were correctly predicted.\n",
    "   - **Importance**: High recall reduces false negatives.\n",
    "\n",
    "#### 4. F1-Score (Harmonic Mean of Precision and Recall) \n",
    "   - **Formula**:  \n",
    "    \n",
    "     $${F1-Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "    \n",
    "   - **Interpretation**: A balanced metric that considers both precision and recall. Useful for imbalanced datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### Evaluating Model Performance\n",
    "- **Single Accuracy**:\n",
    "  - Simpler but less informative for imbalanced datasets.\n",
    "  - Example: If 90% of data belongs to one class, a model predicting only that class achieves 90% accuracy without identifying any minority-class samples.\n",
    "\n",
    "- **Confusion Matrix**:\n",
    "  - Provides a detailed breakdown of correct and incorrect predictions.\n",
    "  - Helps calculate additional metrics like precision, recall, and F1-score to better evaluate model performance in complex scenarios.\n",
    "\n",
    "---\n",
    "\n",
    "### Example of Imbalanced Dataset\n",
    "- Dataset: 100 samples, 95 negatives, 5 positives.\n",
    "- Model predicts all negatives:\n",
    "  - Accuracy: 95% (good score).\n",
    "  - Recall: 0% for the positive class (poor performance).\n",
    "  \n",
    "Using only accuracy hides the model's inability to predict the minority class.\n",
    "\n",
    "---\n",
    "\n",
    "A **confusion matrix** and derived metrics like **precision**, **recall**, and **F1-score** provide a more comprehensive view of model performance than accuracy alone, especially in cases with imbalanced datasets or when certain types of errors (e.g., false positives or false negatives) are more costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "task1": {
     "name": "task1",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert data.shape[0] == 41188, 'Dataset should have 41188 rows. Please check if you loaded the correct dataset.'\n>>> assert data.shape[1] == 21, 'Dataset should have 21 columns. Please check if you loaded the correct dataset.'\n>>> assert head_data.shape[0] == 5, \"The 'head_data' should return exactly 5 rows. Ensure you are using the 'head()' function correctly.\"\n>>> assert data_description.equals(data.describe()), \"The function used to get summary of statistics is not correct. Ensure you use the 'describe()' method.\"\n",
         "hidden": false,
         "locked": false,
         "points": 5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "task2": {
     "name": "task2",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(missing_values, pd.Series), 'The output of `isnull().sum()` should be a pandas Series.'\n>>> assert missing_values.sum() == 0, 'Missing values count should be non-negative.'\n>>> assert len(data_shape) == 2, 'The shape should have two dimensions (rows, columns).'\n>>> assert data_shape[0] == 41188, 'The DataFrame should have at least one row.'\n>>> assert data_shape[1] == 21, 'The DataFrame should have at least one column.'\n>>> assert len(education_categories) == 8, 'The `education` column should contain 8 unique categories.'\n>>> assert list(education_categories) == ['basic.4y', 'unknown', 'university.degree', 'high.school', 'basic.9y', 'professional.course', 'basic.6y', 'illiterate']\n",
         "hidden": false,
         "locked": false,
         "points": 5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "task3": {
     "name": "task3",
     "points": 20,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert y_value_counts.sum() == 41188, 'The sum of value counts should equal the total number of rows.'\n>>> assert y_value_counts[0] == 36548, 'The number of 0 class is not correct.'\n>>> assert y_value_counts[1] == 4640, 'The number of 1 class is not correct.'\n",
         "hidden": false,
         "locked": false,
         "points": 5
        },
        {
         "code": ">>> assert count_no_sub == 36548, 'Count of non-subscriptions is wrong.'\n>>> assert count_sub == 4640, 'Count of subscriptions is wrong.'\n>>> assert count_no_sub + count_sub == 41188, 'The sum of subscriptions and non-subscriptions should equal the total number of rows.'\n>>> assert pct_of_no_sub == 36548 / 41188, 'The value of pct_of_no_sub is wrong'\n>>> assert pct_of_sub == 4640 / 41188, 'The value of pct_of_sub is wrong'\n>>> assert np.isclose(pct_of_no_sub + pct_of_sub, 1), 'The sum of percentages should equal 100%.'\n",
         "hidden": false,
         "locked": false,
         "points": 5
        },
        {
         "code": ">>> expected_grouped_means = data.groupby('y').mean(numeric_only=True)\n>>> assert grouped_means.equals(expected_grouped_means), 'The output of `groupby().mean()` is not correct'\n>>> assert isinstance(grouped_means, pd.DataFrame), 'The output of `groupby().mean()` should be a pandas DataFrame.'\n>>> assert 'y' not in grouped_means.columns, \"The grouped DataFrame should not contain the 'y' column.\"\n",
         "hidden": false,
         "locked": false,
         "points": 5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "task5": {
     "name": "task5",
     "points": 30,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert X.shape[0] == 41188, 'The number of rows in X should match the number of rows in data_final.'\n>>> assert y.shape[0] == 41188, 'The number of rows in y should match the number of rows in data_final.'\n>>> assert 'y' not in X.columns, \"'y' column should not be present in X.\"\n",
         "hidden": false,
         "locked": false,
         "points": 5
        },
        {
         "code": ">>> assert len(majority) == 25622, 'The majority class is empty.'\n>>> assert len(minority) == 3209, 'The minority class is empty.'\n>>> assert majority.shape[0] + minority.shape[0] == train_data.shape[0], 'The majority and minority classes do not sum up to the training data size.'\n>>> assert minority_upsampled.shape[0] == majority.shape[0], 'The number of upsampled minority class samples is not equal to the majority class.'\n>>> assert minority_upsampled.shape[0] > minority.shape[0], 'The minority class was not upsampled properly.'\n>>> assert majority.equals(train_data[train_data['y'] == 0]), 'wrong majority'\n>>> assert minority.equals(train_data[train_data['y'] == 1]), 'wrong minority'\n",
         "hidden": false,
         "locked": false,
         "points": 5
        },
        {
         "code": ">>> assert upsampled_data.shape[0] == majority.shape[0] + minority_upsampled.shape[0], 'The concatenated data has incorrect size.'\n>>> assert 'y' in upsampled_data.columns, \"The 'y' column is missing in the upsampled data.\"\n>>> assert 'y' not in X_train.columns, \"'y' should not be present in X_train.\"\n>>> assert y_train.shape[0] == upsampled_data.shape[0], \"The size of y_train doesn't match the number of rows in the upsampled data.\"\n>>> assert len(y_train[y_train == 0]) == len(y_train[y_train == 1]), 'The class distribution after upsampling is not balanced.'\n>>> assert set(y_train).issubset({0, 1}), 'The y_train variable contains values outside the expected 0 and 1 classes.'\n",
         "hidden": false,
         "locked": false,
         "points": 10
        },
        {
         "code": ">>> assert X_train_scaled.shape == X_train.shape, 'X_train_scaled should have the same shape as X_train'\n>>> assert X_test_scaled.shape == X_test.shape, 'X_test_scaled should have the same shape as X_test'\n>>> assert np.allclose(X_train_scaled.mean(axis=0), 0, atol=1e-07), 'Mean of scaled features in X_train_scaled should be close to 0'\n>>> assert np.allclose(X_train_scaled.std(axis=0), 1, atol=1e-07), 'Standard deviation of scaled features in X_train_scaled should be close to 1'\n>>> assert np.allclose((X_test - scaler.mean_) / scaler.scale_, X_test_scaled, atol=1e-07), 'X_test_scaled should be consistent with the scaling applied during fit on X_train'\n",
         "hidden": false,
         "locked": false,
         "points": 5
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "task6": {
     "name": "task6",
     "points": 20,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hasattr(logreg, 'coef_'), 'The logistic regression model was not trained properly.'\n>>> assert round(accuracy, 2) == 0.85, 'The accuracy value is not correct.'\n>>> assert len(y_pred) == len(y_test), 'The number of predictions should match the number of test samples.'\n",
         "hidden": false,
         "locked": false,
         "points": 20
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "task7": {
     "name": "task7",
     "points": 30,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert hyperparameters == {'penalty': ['l1', 'l2', 'elasticnet', 'none'], 'C': [0.01, 0.1, 1, 10]}, 'Hyperparameter dictionary is incorrect.'\n>>> assert isinstance(lgr, LogisticRegression), 'Logistic Regression model not instantiated correctly.'\n>>> assert isinstance(grid_search, GridSearchCV), 'GridSearchCV is not instantiated correctly.'\n>>> assert grid_search.param_grid == hyperparameters, 'Parameter grid is incorrect in GridSearchCV.'\n",
         "hidden": false,
         "locked": false,
         "points": 10
        },
        {
         "code": ">>> assert grid_search.best_params_ == best_params, 'Best parameters do not match GridSearchCV output.'\n>>> assert grid_search.best_score_ == best_score, 'Best score does not match GridSearchCV output.'\n>>> assert grid_search.best_estimator_ == best_model, 'Best model does not match GridSearchCV output.'\n",
         "hidden": false,
         "locked": false,
         "points": 10
        },
        {
         "code": ">>> assert y_pred.shape[0] == y_test.shape[0], 'Predictions shape does not match test data shape.'\n>>> assert conf_matrix[0][0] == 9296 or conf_matrix[0][1] == 1630 or conf_matrix[1][0] == 209 or (conf_matrix[1][1] == 1222), 'The values in confusion matrix are not correct.'\n",
         "hidden": false,
         "locked": false,
         "points": 10
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
